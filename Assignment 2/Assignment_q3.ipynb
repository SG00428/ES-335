{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q3)  Referring to the same notebook, explain why Sklearn's linear regression implementation is robust against multicollinearity. Dive deep into Sklearn's code and explain in depth the methodology used in sklearn's implementation.\n",
        "\n",
        "\n",
        "Ans -\n",
        "Scikit-learn's Linear Regression implementation is robust against multicollinearity due to the use of the Singular Value Decomposition (SVD) method for solving the normal equations. The SVD method is used in the fit method of the LinearRegression class in scikit-learn.\n",
        "\n",
        "Here's a brief overview of the methodology used in scikit-learn's implementation:\n",
        "\n",
        "1. Data Preprocessing: Before fitting the model, scikit-learn's LinearRegression class automatically centers the data by subtracting the mean from each feature and scales the data by dividing by the standard deviation. This is done to ensure that all features have a mean of 0 and a standard deviation of 1, which can help improve the numerical stability of the SVD method.\n",
        "\n",
        " 2 .Singular Value Decomposition (SVD): The SVD method is used to decompose the  matrix X into three matrices: U, S, and Vt. The matrix S is a diagonal matrix containing the singular values of X, and U and Vt are orthogonal matrices. The SVD decomposition of X is given by X = U @ S @ Vt.\n",
        "\n",
        " 3. Pseudo-Inverse: The pseudo-inverse of X is calculated as X_pinv = Vt.T @ S_pinv @ U.T, where S_pinv is the pseudo-inverse of S. The pseudo-inverse of S is obtained by taking the reciprocal of each non-zero singular value and transposing the resulting matrix.\n",
        "\n",
        " 4. Coefficients Calculation: The coefficients of the linear regression model are calculated as theta = X_pinv @ y, where y is the target variable.\n",
        "\n",
        " 5. Prediction: Once the coefficients are calculated, predictions can be made using the formula y_pred = X @ theta.\n",
        "\n",
        "The use of the SVD method for solving the normal equations makes it more  robust against multicollinearity because it can handle singular or nearly singular matrices. The SVD method is numerically stable and can accurately compute the pseudo-inverse even when the matrix X is ill-conditioned or has a high condition number due to multicollinearity.\n",
        "\n",
        " the methodology used in sklearn's implementation\n",
        "\n",
        " Scikit-learn's Linear Regression implementation is based on the Ordinary Least Squares (OLS) method, The OLS method aims to minimize the sum of squared differences between the observed and predicted values of the target variable.\n",
        "\n",
        "\n",
        " Here's a detailed explanation of the methodology used in scikit-learn's Linear Regression implementation:\n",
        "\n",
        "Data Preprocessing:  As mentioned above Before fitting the model, scikit-learn's LinearRegression class automatically centers the data by subtracting the mean from each feature and scales the data by dividing by the standard deviation. This is done to ensure that all features have a mean of 0 and a standard deviation of 1, which can help improve the numerical stability of the algorithm.\n",
        "\n",
        "Matrix Formulation: The linear regression problem can be formulated in matrix form as y = X @ theta + epsilon, where y is the target variable, X is the design matrix containing the features, theta is the vector of coefficients to be estimated, and epsilon is the error term.\n",
        "\n",
        "Normal Equation: The OLS method solves for the coefficients theta by minimizing the sum of squared differences between the observed and predicted values of y. This can be expressed as minimize ||y - X @ theta||^2, where ||.|| denotes the Euclidean norm.\n",
        "\n",
        "Solving the Normal Equation: The normal equation for the OLS method is given by X.T @ X @ theta = X.T @ y. This equation can be solved for theta using various methods, such as matrix inversion or the Singular Value Decomposition (SVD) method.\n",
        "\n",
        "Matrix Inversion: One way to solve the normal equation is by directly computing the inverse of the matrix X.T @ X and multiplying it with X.T @ y. However, this approach can be numerically unstable and inefficient for large matrices.\n",
        "\n",
        "Singular Value Decomposition (SVD):  As mentioned earlier also Scikit-learn's Linear Regression implementation uses the SVD method for solving the normal equation. The SVD method decomposes the matrix X into three matrices U, S, and Vt, such that X = U @ S @ Vt. The matrix S is a diagonal matrix containing the singular values of X, and U and Vt are orthogonal matrices. The SVD decomposition of X is used to compute the pseudo-inverse of X, which is then used to calculate the coefficients theta.\n",
        "\n",
        "Pseudo-Inverse: The pseudo-inverse of X is calculated as X_pinv = Vt.T @ S_pinv @ U.T, where S_pinv is the pseudo-inverse of S. The pseudo-inverse of S is obtained by taking the reciprocal of each non-zero singular value and transposing the resulting matrix.\n",
        "\n",
        "Coefficients Calculation: The coefficients of the linear regression model are calculated as theta = X_pinv @ y, where y is the target variable.\n",
        "\n",
        "Prediction: Once the coefficients are calculated, predictions can be made using the formula y_pred = X @ theta.\n",
        "\n",
        "Overall, scikit-learn's Linear Regression implementation is based on the OLS method and uses the SVD method for solving the normal equation. This approach is numerically stable and efficient, making it suitable for large datasets and high-dimensional feature spaces."
      ],
      "metadata": {
        "id": "y6xjNmYYL10d"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k1Mh8K5rOGO6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}